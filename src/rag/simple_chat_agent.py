import logging
import os
from pathlib import Path
from typing import Dict, Any

from langchain_core.messages import HumanMessage
from llm import get_gemini_llm, LLMConfig
from rag.retriever import create_hybrid_retriever

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleChatAgent:
    """Simplified chat agent without LangGraph to avoid recursion issues"""
    
    def __init__(self, custom_retriever=None, model_name: str = None):
        """Initialize the Simple Chat Agent"""
        
        # Create LLM
        if model_name is None:
            model_name = LLMConfig.DEFAULT_GEMINI_MODEL
            
        try:
            self.llm = get_gemini_llm(model_name=model_name)
            logger.info(f"Initialized LLM with Gemini model: {model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini LLM: {e}")
            raise
        
        # Store the retriever
        self.retriever = custom_retriever if custom_retriever is not None else self.get_default_retriever()
        
        # Load prompts
        self.prompts = self._load_prompts()
    
    def get_default_retriever(self):
        """Get the default hybrid retriever for KMA regulations"""
        current_dir = Path(__file__).parent.absolute()
        project_root = current_dir.parent.parent
        vector_db_path = os.path.join(project_root, "vector_db")
        data_dir = os.path.join(project_root, "data")
        
        hybrid_retriever, _ = create_hybrid_retriever(vector_db_path=vector_db_path, data_dir=data_dir)
        return hybrid_retriever
    
    def _load_prompts(self):
        """Load prompts from files"""
        prompts = {}
        prompts_dir = os.path.join(os.path.dirname(__file__), "prompts")
        
        # Try to load detailed prompt first
        try:
            with open(os.path.join(prompts_dir, "detailed_generate.txt"), "r", encoding="utf-8") as f:
                prompts["generate"] = f.read().strip()
                logger.info("Loaded detailed generate prompt")
        except FileNotFoundError:
            # Try standard prompt
            try:
                with open(os.path.join(prompts_dir, "generate.txt"), "r", encoding="utf-8") as f:
                    prompts["generate"] = f.read().strip()
                    logger.info("Loaded standard generate prompt")
            except FileNotFoundError:
                # Enhanced detailed prompt for file content
                prompts["generate"] = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu.

Nhi·ªám v·ª•: H√£y ph√¢n t√≠ch k·ªπ th√¥ng tin ƒë∆∞·ª£c cung c·∫•p v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt, ƒë·∫ßy ƒë·ªß v√† h·ªØu √≠ch.

C√¢u h·ªèi: {question}

Th√¥ng tin t·ª´ t√†i li·ªáu:
{context}

Y√™u c·∫ßu tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi v·ªõi th√¥ng tin c·ª• th·ªÉ t·ª´ t√†i li·ªáu
2. Cung c·∫•p chi ti·∫øt v√† v√≠ d·ª• n·∫øu c√≥ trong t√†i li·ªáu
3. N·∫øu c√≥ nhi·ªÅu th√¥ng tin li√™n quan, h√£y t·ªï ch·ª©c th√†nh c√°c ƒëi·ªÉm r√µ r√†ng
4. N·∫øu th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√™u r√µ nh·ªØng g√¨ c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c
5. S·ª≠ d·ª•ng ng√¥n ng·ªØ r√µ r√†ng, d·ªÖ hi·ªÉu v√† t·ª± nhi√™n

Tr·∫£ l·ªùi chi ti·∫øt:"""
                logger.info("Using fallback detailed prompt")
        
        return prompts
    
    def chat(self, message: str) -> str:
        """Process a chat message and return detailed response"""
        try:
            logger.info(f"Processing query: {message}")
            
            # Retrieve relevant documents (more documents for better context)
            docs = self.retriever.get_relevant_documents(message)
            
            # Use more context for detailed answers
            context_docs = docs[:8]  # Increase from 5 to 8 for more context
            context = "\n\n---\n\n".join([
                f"ƒêo·∫°n {i+1}:\n{doc.page_content}" 
                for i, doc in enumerate(context_docs)
            ])
            
            # Enhanced prompt for detailed responses
            if context.strip():
                # Add context about the query type for better responses
                enhanced_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, h√£y ph√¢n t√≠ch k·ªπ c√¢u h·ªèi v√† th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.

üéØ C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi: {message}

üìö Th√¥ng tin t·ª´ t√†i li·ªáu (ƒë∆∞·ª£c chia th√†nh c√°c ƒëo·∫°n):
{context}

üìù H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
‚Ä¢ ƒê·ªçc k·ªπ t·∫•t c·∫£ c√°c ƒëo·∫°n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
‚Ä¢ T·ªïng h·ª£p v√† ph√¢n t√≠ch ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß nh·∫•t
‚Ä¢ S·∫Øp x·∫øp th√¥ng tin theo logic, chia th√†nh c√°c ph·∫ßn r√µ r√†ng
‚Ä¢ Cung c·∫•p v√≠ d·ª• c·ª• th·ªÉ t·ª´ t√†i li·ªáu n·∫øu c√≥
‚Ä¢ S·ª≠ d·ª•ng bullet points ho·∫∑c s·ªë th·ª© t·ª± khi ph√π h·ª£p
‚Ä¢ N·∫øu c√≥ nhi·ªÅu kh√≠a c·∫°nh, h√£y tr√¨nh b√†y t·ª´ng kh√≠a c·∫°nh m·ªôt c√°ch c√≥ h·ªá th·ªëng

üí¨ C√¢u tr·∫£ l·ªùi chi ti·∫øt:"""
            else:
                enhanced_prompt = f"""Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë√£ upload ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: "{message}"

Vui l√≤ng th·ª≠:
‚Ä¢ ƒê·∫∑t c√¢u h·ªèi kh√°c li√™n quan ƒë·∫øn n·ªôi dung t√†i li·ªáu
‚Ä¢ S·ª≠ d·ª•ng t·ª´ kh√≥a kh√°c c√≥ th·ªÉ c√≥ trong t√†i li·ªáu
‚Ä¢ Ki·ªÉm tra l·∫°i xem t√†i li·ªáu c√≥ ch·ª©a th√¥ng tin b·∫°n ƒëang t√¨m kh√¥ng

T√¥i s·∫Ω c·ªë g·∫Øng tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t: {message}"""
            
            response = self.llm.invoke([{"role": "user", "content": enhanced_prompt}])
            
            # Post-process response to add more details if needed
            answer = response.content
            
            # Add source information at the end
            if context.strip() and len(context_docs) > 0:
                answer += f"\n\nüìã *Th√¥ng tin ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ {len(context_docs)} ƒëo·∫°n li√™n quan trong t√†i li·ªáu.*"
            
            logger.info("Detailed response generated successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Error in chat processing: {str(e)}")
            return f"‚ùå Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}\n\nVui l√≤ng th·ª≠ l·∫°i ho·∫∑c ƒë·∫∑t c√¢u h·ªèi kh√°c."


async def process_simple_query(query: str, retriever=None, llm=None) -> Dict[str, Any]:
    """Simple query processing function without LangGraph"""
    try:
        # Create agent
        agent = SimpleChatAgent(custom_retriever=retriever)
        
        # Process query
        answer = agent.chat(query)
        
        # Get sources
        docs = agent.retriever.get_relevant_documents(query)
        sources = [doc.page_content for doc in docs[:3]]
        
        return {
            "answer": answer,
            "sources": sources,
            "source_type": "simple_agent"
        }
        
    except Exception as e:
        logger.error(f"Error in simple query processing: {str(e)}")
        return {
            "answer": f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
            "sources": [],
            "source_type": "error"
        }
